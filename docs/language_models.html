<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>dhivehi_nlp.language_models API documentation</title>
<meta name="description" content="Create language models to predict future additions. Language models will give
probability based on selected ngram. An ngram is a contiguous sequence …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>dhivehi_nlp.language_models</code></h1>
</header>
<section id="section-intro">
<p>Create language models to predict future additions. Language models will give
probability based on selected ngram. An ngram is a contiguous sequence of n
tokens from the given input text.</p>
<p>ދީފައިވާ ލިޔުމުގައިވާ ބަސްތަކަށް ބެލުމަށްފަހު ފަހުން މިލިޔުމަށް އައިސްދާނެ ބަސްތައް ލަފާކުރާނެ މޮޑެލްއެއް އުފެއްދުން</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Create language models to predict future additions. Language models will give
probability based on selected ngram. An ngram is a contiguous sequence of n
tokens from the given input text.

ދީފައިވާ ލިޔުމުގައިވާ ބަސްތަކަށް ބެލުމަށްފަހު ފަހުން މިލިޔުމަށް އައިސްދާނެ ބަސްތައް ލަފާކުރާނެ މޮޑެލްއެއް އުފެއްދުން 
&#34;&#34;&#34;

from dhivehi_nlp._helpers import _db_connect
from dhivehi_nlp import tokenizer


def ngrams(text: str, n: int) -&gt; list:
    &#34;&#34;&#34;
    Returns a list of dicts of the ngrams in the text along with their count.
    The ngram is based on the n value provided. If n = 1, the resulting dict
    will have unigrams. If n = 2, bigrams and so on.

    އެންގްރާމް ތަކުގެ ލިސްޓެއް އަނބުރާދޭނެއެވެ. މީގައި އެންގްރާމް ލިޔުމުން ފެނުނު އަދަދު ވެސް ހުންނާނެއެވެ

    &gt;&gt;&gt; text = &#34;ބުނެފައި އަދި އިތުރު. ބުނެފައި އަދި އިތުރުކަމެއް&#34;
    &gt;&gt;&gt; ngrams(text, 2)
    [
        {&#34;gram&#34;: (&#34;ބުނެފައި&#34;, &#34;އަދި&#34;), &#34;count&#34;: 2},
        {&#34;gram&#34;: (&#34;އަދި&#34;, &#34;އިތުރު&#34;), &#34;count&#34;: 1},
        {&#34;gram&#34;: (&#34;އަދި&#34;, &#34;އިތުރުކަމެއް&#34;), &#34;count&#34;: 1},
    ]
    &#34;&#34;&#34;
    if n == 1:
        grams = tokenizer.word_tokenize(text, removePunctuation=True)
    else:
        sentences = tokenizer.sentence_tokenize(text)
        grams = []
        for sentence in sentences:
            tokens = tokenizer.word_tokenize(sentence, removePunctuation=True)
            grams_sentence = [
                tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1)
            ]
            grams = grams + grams_sentence
    grams_counted = []
    for gram in set(grams):
        counted = {&#34;gram&#34;: gram, &#34;count&#34;: grams.count(gram)}
        grams_counted.append(counted)
    return grams_counted


def model(text: str, n: int) -&gt; list:
    &#34;&#34;&#34;
    Returns a list of dicts of a word or phrase in the text and the probability
    of the of the word or phrase appearing in the text.
    The ngram is based on the n value provided. If n = 1, the resulting dict
    will have unigrams. If n = 2, bigrams and so on.

    ބަހެެއް ނުވަތަ ބަސްތަކެއް ލިޔުމުން ފެންނާނެ ކަމުގެ ޕްރޮބަބިލިޓީ ތަކުގެ ލިސްޓެއް އަނބުރާދޭނެއެވެ

    &gt;&gt;&gt; text = &#34;ބުނެފައި އަދި އިތުރު. ބުނެފައި އަދި އިތުރުކަމެއް&#34;
    &gt;&gt;&gt; model(text, 3)
    [
        {&#34;gram&#34;: (&#34;ބުނެފައި&#34;, &#34;އަދި&#34;, &#34;އިތުރު&#34;), &#34;probability&#34;: 0.5},
        {&#34;gram&#34;: (&#34;ބުނެފައި&#34;, &#34;އަދި&#34;, &#34;އިތުރުކަމެއް&#34;), &#34;probability&#34;: 0.5},
    ]
    &#34;&#34;&#34;
    probabilities = []
    if n == 1:
        grams = ngrams(text, 1)
        tokens = tokenizer.word_tokenize(text)
        for i in grams:
            probability = {&#34;gram&#34;: i[&#34;gram&#34;], &#34;probability&#34;: i[&#34;count&#34;] / len(tokens)}
            probabilities.append(probability)
    else:
        grams = ngrams(text, n)
        grams_minus = ngrams(text, n - 1)
        for gram in grams:
            if n == 2:
                gram_search = gram[&#34;gram&#34;][0]
            else:
                gram_search = gram[&#34;gram&#34;][: n - 1]
            for g in grams_minus:
                if g[&#34;gram&#34;] == gram_search:
                    probability = {
                        &#34;gram&#34;: gram[&#34;gram&#34;],
                        &#34;probability&#34;: gram[&#34;count&#34;] / g[&#34;count&#34;],
                    }
                    probabilities.append(probability)
    return probabilities


def news_model_predict(word: str, max_output=10) -&gt; list:
    &#34;&#34;&#34;Predict the next word using a model based on news bigrams. Model was made
    from a 10000 random sample from 132029 news articles obtained from sun.mv.
    Returns a list of dicts containing the predicted word and the probability
    of that word occuring after the input word, ordered by probability in
    descending order.
    The max_output keyword argument determines the size of the return list and
    is set to 10 by default if argument is not given.
    If there are no bigrams containing the input word as the first word, None is
    returned.

    ސަން އެމްވީގެ ހަބަރު ތަކުން ހަދާފައިވާ މޮޑެލް ބޭނުން ކޮށްގެން، ދީފައިވާ އަކުރަށްފަހު އަންނާނެ އަކުރު ލަފާ ކުރާނެ އެވެ

    &gt;&gt;&gt; text = &#39;ވަނަ&#39;
    &gt;&gt;&gt; news_model_predict(text, 3)
    [
        {&#39;prediction&#39;: &#39;އަހަރު&#39;, &#39;probability&#39;: 0.18434617471513837},
        {&#39;prediction&#39;: &#39;ދުވަހު&#39;, &#39;probability&#39;: 0.175122083559414},
        {&#39;prediction&#39;: &#39;އަހަރުގެ&#39;, &#39;probability&#39;: 0.09603906673901248},
    ]
    &#34;&#34;&#34;
    con = _db_connect()
    cursor = con.cursor()
    query = f&#34;SELECT * FROM news_bigrams WHERE first=&#39;{word}&#39;&#34;
    cursor.execute(query)
    matches = cursor.fetchall()
    con.close()
    if not matches:
        return []
    total = 0
    for i in matches:
        total += i[2]
    results = []
    for i in matches:
        result = {&#34;prediction&#34;: i[1], &#34;probability&#34;: i[2] / total}
        results.append(result)
    if len(results) &gt; max_output:
        results = results[:max_output]
    return results</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="dhivehi_nlp.language_models.model"><code class="name flex">
<span>def <span class="ident">model</span></span>(<span>text: str, n: int) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a list of dicts of a word or phrase in the text and the probability
of the of the word or phrase appearing in the text.
The ngram is based on the n value provided. If n = 1, the resulting dict
will have unigrams. If n = 2, bigrams and so on.</p>
<p>ބަހެެއް ނުވަތަ ބަސްތަކެއް ލިޔުމުން ފެންނާނެ ކަމުގެ ޕްރޮބަބިލިޓީ ތަކުގެ ލިސްޓެއް އަނބުރާދޭނެއެވެ</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; text = &quot;ބުނެފައި އަދި އިތުރު. ބުނެފައި އަދި އިތުރުކަމެއް&quot;
&gt;&gt;&gt; model(text, 3)
[
    {&quot;gram&quot;: (&quot;ބުނެފައި&quot;, &quot;އަދި&quot;, &quot;އިތުރު&quot;), &quot;probability&quot;: 0.5},
    {&quot;gram&quot;: (&quot;ބުނެފައި&quot;, &quot;އަދި&quot;, &quot;އިތުރުކަމެއް&quot;), &quot;probability&quot;: 0.5},
]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model(text: str, n: int) -&gt; list:
    &#34;&#34;&#34;
    Returns a list of dicts of a word or phrase in the text and the probability
    of the of the word or phrase appearing in the text.
    The ngram is based on the n value provided. If n = 1, the resulting dict
    will have unigrams. If n = 2, bigrams and so on.

    ބަހެެއް ނުވަތަ ބަސްތަކެއް ލިޔުމުން ފެންނާނެ ކަމުގެ ޕްރޮބަބިލިޓީ ތަކުގެ ލިސްޓެއް އަނބުރާދޭނެއެވެ

    &gt;&gt;&gt; text = &#34;ބުނެފައި އަދި އިތުރު. ބުނެފައި އަދި އިތުރުކަމެއް&#34;
    &gt;&gt;&gt; model(text, 3)
    [
        {&#34;gram&#34;: (&#34;ބުނެފައި&#34;, &#34;އަދި&#34;, &#34;އިތުރު&#34;), &#34;probability&#34;: 0.5},
        {&#34;gram&#34;: (&#34;ބުނެފައި&#34;, &#34;އަދި&#34;, &#34;އިތުރުކަމެއް&#34;), &#34;probability&#34;: 0.5},
    ]
    &#34;&#34;&#34;
    probabilities = []
    if n == 1:
        grams = ngrams(text, 1)
        tokens = tokenizer.word_tokenize(text)
        for i in grams:
            probability = {&#34;gram&#34;: i[&#34;gram&#34;], &#34;probability&#34;: i[&#34;count&#34;] / len(tokens)}
            probabilities.append(probability)
    else:
        grams = ngrams(text, n)
        grams_minus = ngrams(text, n - 1)
        for gram in grams:
            if n == 2:
                gram_search = gram[&#34;gram&#34;][0]
            else:
                gram_search = gram[&#34;gram&#34;][: n - 1]
            for g in grams_minus:
                if g[&#34;gram&#34;] == gram_search:
                    probability = {
                        &#34;gram&#34;: gram[&#34;gram&#34;],
                        &#34;probability&#34;: gram[&#34;count&#34;] / g[&#34;count&#34;],
                    }
                    probabilities.append(probability)
    return probabilities</code></pre>
</details>
</dd>
<dt id="dhivehi_nlp.language_models.news_model_predict"><code class="name flex">
<span>def <span class="ident">news_model_predict</span></span>(<span>word: str, max_output=10) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>Predict the next word using a model based on news bigrams. Model was made
from a 10000 random sample from 132029 news articles obtained from sun.mv.
Returns a list of dicts containing the predicted word and the probability
of that word occuring after the input word, ordered by probability in
descending order.
The max_output keyword argument determines the size of the return list and
is set to 10 by default if argument is not given.
If there are no bigrams containing the input word as the first word, None is
returned.</p>
<p>ސަން އެމްވީގެ ހަބަރު ތަކުން ހަދާފައިވާ މޮޑެލް ބޭނުން ކޮށްގެން، ދީފައިވާ އަކުރަށްފަހު އަންނާނެ އަކުރު ލަފާ ކުރާނެ އެވެ</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; text = 'ވަނަ'
&gt;&gt;&gt; news_model_predict(text, 3)
[
    {'prediction': 'އަހަރު', 'probability': 0.18434617471513837},
    {'prediction': 'ދުވަހު', 'probability': 0.175122083559414},
    {'prediction': 'އަހަރުގެ', 'probability': 0.09603906673901248},
]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def news_model_predict(word: str, max_output=10) -&gt; list:
    &#34;&#34;&#34;Predict the next word using a model based on news bigrams. Model was made
    from a 10000 random sample from 132029 news articles obtained from sun.mv.
    Returns a list of dicts containing the predicted word and the probability
    of that word occuring after the input word, ordered by probability in
    descending order.
    The max_output keyword argument determines the size of the return list and
    is set to 10 by default if argument is not given.
    If there are no bigrams containing the input word as the first word, None is
    returned.

    ސަން އެމްވީގެ ހަބަރު ތަކުން ހަދާފައިވާ މޮޑެލް ބޭނުން ކޮށްގެން، ދީފައިވާ އަކުރަށްފަހު އަންނާނެ އަކުރު ލަފާ ކުރާނެ އެވެ

    &gt;&gt;&gt; text = &#39;ވަނަ&#39;
    &gt;&gt;&gt; news_model_predict(text, 3)
    [
        {&#39;prediction&#39;: &#39;އަހަރު&#39;, &#39;probability&#39;: 0.18434617471513837},
        {&#39;prediction&#39;: &#39;ދުވަހު&#39;, &#39;probability&#39;: 0.175122083559414},
        {&#39;prediction&#39;: &#39;އަހަރުގެ&#39;, &#39;probability&#39;: 0.09603906673901248},
    ]
    &#34;&#34;&#34;
    con = _db_connect()
    cursor = con.cursor()
    query = f&#34;SELECT * FROM news_bigrams WHERE first=&#39;{word}&#39;&#34;
    cursor.execute(query)
    matches = cursor.fetchall()
    con.close()
    if not matches:
        return []
    total = 0
    for i in matches:
        total += i[2]
    results = []
    for i in matches:
        result = {&#34;prediction&#34;: i[1], &#34;probability&#34;: i[2] / total}
        results.append(result)
    if len(results) &gt; max_output:
        results = results[:max_output]
    return results</code></pre>
</details>
</dd>
<dt id="dhivehi_nlp.language_models.ngrams"><code class="name flex">
<span>def <span class="ident">ngrams</span></span>(<span>text: str, n: int) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a list of dicts of the ngrams in the text along with their count.
The ngram is based on the n value provided. If n = 1, the resulting dict
will have unigrams. If n = 2, bigrams and so on.</p>
<p>އެންގްރާމް ތަކުގެ ލިސްޓެއް އަނބުރާދޭނެއެވެ. މީގައި އެންގްރާމް ލިޔުމުން ފެނުނު އަދަދު ވެސް ހުންނާނެއެވެ</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; text = &quot;ބުނެފައި އަދި އިތުރު. ބުނެފައި އަދި އިތުރުކަމެއް&quot;
&gt;&gt;&gt; ngrams(text, 2)
[
    {&quot;gram&quot;: (&quot;ބުނެފައި&quot;, &quot;އަދި&quot;), &quot;count&quot;: 2},
    {&quot;gram&quot;: (&quot;އަދި&quot;, &quot;އިތުރު&quot;), &quot;count&quot;: 1},
    {&quot;gram&quot;: (&quot;އަދި&quot;, &quot;އިތުރުކަމެއް&quot;), &quot;count&quot;: 1},
]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ngrams(text: str, n: int) -&gt; list:
    &#34;&#34;&#34;
    Returns a list of dicts of the ngrams in the text along with their count.
    The ngram is based on the n value provided. If n = 1, the resulting dict
    will have unigrams. If n = 2, bigrams and so on.

    އެންގްރާމް ތަކުގެ ލިސްޓެއް އަނބުރާދޭނެއެވެ. މީގައި އެންގްރާމް ލިޔުމުން ފެނުނު އަދަދު ވެސް ހުންނާނެއެވެ

    &gt;&gt;&gt; text = &#34;ބުނެފައި އަދި އިތުރު. ބުނެފައި އަދި އިތުރުކަމެއް&#34;
    &gt;&gt;&gt; ngrams(text, 2)
    [
        {&#34;gram&#34;: (&#34;ބުނެފައި&#34;, &#34;އަދި&#34;), &#34;count&#34;: 2},
        {&#34;gram&#34;: (&#34;އަދި&#34;, &#34;އިތުރު&#34;), &#34;count&#34;: 1},
        {&#34;gram&#34;: (&#34;އަދި&#34;, &#34;އިތުރުކަމެއް&#34;), &#34;count&#34;: 1},
    ]
    &#34;&#34;&#34;
    if n == 1:
        grams = tokenizer.word_tokenize(text, removePunctuation=True)
    else:
        sentences = tokenizer.sentence_tokenize(text)
        grams = []
        for sentence in sentences:
            tokens = tokenizer.word_tokenize(sentence, removePunctuation=True)
            grams_sentence = [
                tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1)
            ]
            grams = grams + grams_sentence
    grams_counted = []
    for gram in set(grams):
        counted = {&#34;gram&#34;: gram, &#34;count&#34;: grams.count(gram)}
        grams_counted.append(counted)
    return grams_counted</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="dhivehi_nlp" href="index.html">dhivehi_nlp</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="dhivehi_nlp.language_models.model" href="#dhivehi_nlp.language_models.model">model</a></code></li>
<li><code><a title="dhivehi_nlp.language_models.news_model_predict" href="#dhivehi_nlp.language_models.news_model_predict">news_model_predict</a></code></li>
<li><code><a title="dhivehi_nlp.language_models.ngrams" href="#dhivehi_nlp.language_models.ngrams">ngrams</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>